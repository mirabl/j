Prob: reflex B<br>1. linked list (2)<br>2. subarray sum<br>3. search<br>4. palindrome (2)<br>5. 1D/2D problem	1. two pointers, reverse<br>2. prefix sum<br>3. sort before<br>4. look at reverse string (LCS), windowed DP<br>5. DP
Prob: reflex C<br>1. get low complexity for conflicting ops<br>2. many strings/dictionary<br>3. graph (2)<br>4. in-place<br>5. small size set	1. use <b>multiple DS</b> (reverse index)<br>2. use <b>trie</b><br>3. connected co, MST<br>4. several <b>passes</b><br>5. use <b>bitmask</b>
Prob: reflex D<br>1. find max less than k (min/more)<br>2. Graph all paths<br>3. count s.t. = K<br>4. count to the right of array	1. <b>BST</b> with upper/lower_bound<br>2. BF backtracking if possible, maybe TSP DP<br>3. count <= K - <= K - 1<br>4. D&C mergesort-like
Prob: stable matching problem (def, solution idea, complex)	= <b>stable marriage</b> problem<br><br>n men, n women<br>every one has <b>order of preference</b> over partners<br>marry such that there is not (1 m, 1 w) who would rather be together<br><br>solution: <b>rounds</b> with <b>proposals/switches (deferred acceptance)</b> O(n^2)
Prob: view from above (stacked segments of different color and height)	sort enter/exit list of segments<br>maintain <b>BST</b> (height, color)
Prob:<br>0-1 Knapsack: description, solution, problem class	max weight, items with values and weights, maximize value<br><br>DP Best[i][w]<br>O(n W)<br>NP-complete
Prob:<br>combination sum: Given a set of candidate numbers (C) (without duplicates) and a target number (T), find all unique combinations in C where the candidate numbers sums to T., <br>same number can be repeated<br>	backtracking (or DP)
SD apps: C numbers	BR 1M/s<br><br>NoSQL cache/persisted: 200To, 1k memcache, 1k couchbase, 1ms 99p, 60M QPS peak<br><br>HDFS: 300 PB, 3k nodes
SD apps: URL shortener	lookup/create<br>read-heavy, load-latency read, metadata store, independent entries, mostly immutable<br>lookup by key<br>data bias+, memory cache<br>key generation service for shortened URLs? maybe offline
SD apps: chat, +protocol	end-to-end encryption? multi device? history on server? ack?<br>push updates: websocket?<br>message delivery queue for offline clients<br>low-latency delivery when recipient online<br>many online clients<br>XMPP
SD apps: docs	metadata+documents<br>can compress text<br>handle conflicts<br>handle offline
SD apps: how many tweets <br>- per second<br>- per day	per second: 6000<br><br>per day: 500 million
SD apps: instagram	features: feed, by user list, upload<br>metadata+blob store<br>read heavy, bias++, immutable<br>image compression pipeline<br>memory cache<br>e.g. FB Haystack for blob store
SD apps: search	- <b>index servers</b> (random sharded on docs)<br>- <b>doc servers</b> (same)<br><br>scatter: build hit list<br>gather: compute relevance order<br><br>docserver: fetch doc from disk, get snippet for keyword
SD apps: youtube	view/upload<br>read heavy but long tail of uploads<br>metadata and blob stores<br>data bias++<br>video processing pipeline
SD components/kafka: 2 Kafka guarantees	at-least-once<br>in order within a partition only
SD components/kafka: 2 difficulties of streaming systems	order<br>exactly once delivery
SD components/kafka: Kafka perf: factors<br>produce, consume, latency	depends on: message size, replication (number and sync/async)<br><br>producer: <b>700K</b> msg/s<br>consumer: <b>1M</b> msg/s<br>end-to-end latency: <b>~3ms</b> 99p
SD components/kafka: Rabbit MQ vs. Kafka 	Rabbit:<br>- canâ€™t rewind<br>- push to consumers<br>- no batching<br>- better for single messages<br>- worse for scale<br>- different topology
SD components/kafka: how many Kafka messages per second at Uber<br><br>how much storage at FB/YT	billions<br><br>500+ PB
SD components/kafka: messaging system semantics	<b>at-least-once</b>: producer client retries when ack timeout or error<br><br><b>at-most-once</b>: no retries<br><br><b>exactly-once</b>: on Kafka, uses sequence numbers
SD components/kafka: why Kafka performance (2)	zero-copy<br><br>batching
SD components/store DS: LSM: MemTable, SSTable<br>stored where, read/writes, maintenance	MemTable in memory<br>SSTable on disk (except idx)<br>writes go to MemTable<br>reads go to MemTable first<br><br>periodically:<br>- MemTable flushed to SSTable<br>- SSTables merged
SD components/store DS: SSTable<br>def, internals, read, write	sorted immutable on disk<br>- Index (in-memory): PK (ex. B-Tree)<br>- Data 64KB block: k/v pairs<br>optimised for reads<br>written once, no in-place changes
SD components/store DS: log structured merge trees (principle, components, good for, compare)	MemTable/SSTable<b>s</b><br>+ disk WAL<br><br>do only <b>sequential write on disk</b><br>thanks to in-memory buffer<br><br>better <b>write throughput</b> than B+<br>good for write-heavy<br><br>read: try all SST (Bloom)
SD components/store: 4 cache strategies	cache aside: app does everything<br><br>write-through: cache interacts with DB<br><br>write-behind/back: app writes to cache and async write to DB<br><br>refresh ahead: auto refresh by cache
SD components/store: 5 techniques to scale RDBMS<br>general/read/writes	general:<br>- federation<br>- denormalization<br>- SQL tuning<br><br>scale reads:<br>- master-slave replication<br><br>scale writes:<br>- sharding
SD components/store: BigQuery/Dremel how it distribute queries	<b>Tree</b> Architecture<br>dispatching queries and aggregating results<br>thousands of machines in a few seconds
SD components/store: BigQuery/Dremel perf for 1T read	with 10K disks and 5K cpus<br><br><b>1 second to read 1TB</b>
SD components/store: BigQuery/Dremel when to use<br>when to use MapReduce	BigQuery: any query, SQL-like, no latency, small output (stats)<br><br>MapReduce: can do any processing, more latency, have to implement all jobs
SD components/store: BigTable scale	thousands of machines, <br>TB memory, <br>PB disk, <br>millions of r/w per second<br>billions of rows<br>thousands of columns
SD components/store: BigTable: data model, API, 2 open source equivalents	1. row (row key)<br>2. column: column families + column qualifier within family<br>3. timestamp<br><br>sparse: no room taken for empty cells<br><br>- lookup by row key, range query<br>- HBase, Cassandra
SD components/store: Cassandra: coordinator	one <b>normal</b> node, can <b>change</b> at any request<br><br>in charge of the <b>whole request</b>:<br>- consistent hash to <b>find nodes</b> to query<br><br><b>responds to client</b> according to config consistency level 
SD components/store: Cassandra: partitioning, replication, consistency	<b>Consistent hashing</b><br><br>Replication factor: <b>copy</b> to <b>RF</b> machines = neighbors in hash ring<br><br>Consistency read/write levels:<br>min numbers of nodes reached by a query<br>(One, Quorum=1+RF/2, All)
SD components/store: Dynamo<br>sharding, quorums, conflict, replica sync	consistent hashing<br>partial quorums for reading and writing<br>conflict detection and read repair via vector clocks<br>gossip for replica synchronization
SD components/store: Master-master replication	Both masters serve reads and writes and coordinate with each other on writes. <br>If either master goes down, the system can continue to operate with both reads and writes.
SD components/store: MySQL default replication	async primary/backup
SD components/store: NoSQL 4 types of stores	key-value: Redis, memcached<br><br>document store: MongoDB, CouchDB, ElasticSearch<br><br>wide-column store: BigTable, HBase, Cassandra<br><br>graph DB: neo4j, flockdb
SD components/store: OLTP, OLAP	online transaction/analytical processing<br>RDBMS vs. datawarehouse<br>low vs high latency
SD components/store: RDBMS one instance perf	no answer, depends on query/HW, if fits in RAM<br><br><b>1 TB</b> total size<br><b>billions</b> of rows<br><b>1K</b> r/w QPS<br><br>latency: SSD 1ms, HD: 20ms
SD components/store: Raid 0, 1, 5	- Raid <b>0</b>: <b>shard</b> (stripe) data on disks. Better <b>perf</b>.<br><br>- Raid <b>1</b>: <b>replicas</b>. N-1 <b>resiliency</b><br><br>- Raid <b>5</b>: 3+ disks + <b>parity</b> disk (spread)<br>supports 1 disk down
SD components/store: advantage/disadvantage of columnar storage, 3 examples	if usage pattern is to read whole table like OLAP/BI<br>but only some columns<br><br>don't read unused columns<br>can use <b>compression</b> to reduce disk size<br>disadv: <b>difficult to update</b><br>Parquet, Redshift, Dremel/BigQuery
SD components/store: in MySQL master-slave, how to ensure read-after-write has most recent value	read from Master
SD components/store: in-memory K/V store perf	on machine with good network<br><br>read/write: 200k/s easily<br><br>latency: 1ms<br><br>no bound on size with sharding but $
SD components/store: replication lag for MySQL	time between read/write to master and replication to all slaves
SD components/store:<br>2 data warehouses products for an analytical processing OLAP	Redshift, BigQuery/Dremel
SD components: ETL	extract data from multiple sources<br><br>clean, join, aggregate, transform into format<br><br>load in warehouse
SD components: Load balancer Geo load-balancing with DNS	some DNS servers have IP range tables and can have different responses for different ranges
SD components: load balancer multi setup (2)	active-passive or active-active
SD components: load balancer routes traffic based on (6)	- random<br>- least load<br>- session/cookies<br>- (weighted) round robin<br>- layer 4<br>- layer 7
